# Evolution of Classification models in NLP

Basically from fine-tuning LMs to prompting LLMs. However, does zero/few shot prompting always work? 
What if you have thousands of classes? 
What if the number of patterns you have are way more than you can fit in a prompt?
To add more: you can have scenarios where you need additional explanations for each class: CoT prompts for each class.

If you have labelled data: is the old way of training a classifier is a good idea? 




### Strategies to select examples for few shot learnings for promting


### Using Class description rather than training examples


### Is fine-tuning still better than prompt selection?
